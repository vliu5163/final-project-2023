# Model 4: Naive Bayes

```{r, echo = FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, error = TRUE, echo = FALSE)
```

## Naive Bayes Model
```{r}
taxi_data <- read.csv('data/2021_taxi_data_mod.csv')
taxi_data <- taxi_data[!is.na(taxi_data$passenger_count),]
taxi_data$tip <- factor(taxi_data$tip)
taxi_data$passenger_count <- factor(taxi_data$passenger_count)
taxi_data$VendorID <- factor(taxi_data$VendorID)
taxi_data$RatecodeID <- factor(taxi_data$RatecodeID)
taxi_data$high_fare <- factor(taxi_data$high_fare)
taxi_data$payment_type <- factor(taxi_data$payment_type)
```

```{r}
library(e1071)
library(tidyverse)
```

For my final model, I wanted to see how Naive Bayes would work. It seems like a pretty simple predictive model, so I wanted to see how it may compare to my other models. For this model, I'll just produce the model with the same features I've been using so far and get the accuracy, sensitivity, and specificity. 

```{r}
set.seed(5293)
n <- nrow(taxi_data)
train <- sample(n, .8*n)
train_dat <- taxi_data[train, ]
test_dat <- taxi_data[-train, ]
```

```{r}
nb_mod <- naiveBayes(tip ~ passenger_count + VendorID + RatecodeID + dropoff_borough + pickup_borough + 
                       season + high_fare + payment_type, data = train_dat)
nb_mod_pred <- predict(nb_mod, test_dat, type = "class")
table(nb_mod_pred, test_dat$tip) 
```
```{r}
library(caret)
test_predicted <- predict(nb_mod, test_dat, type = "class")
test_pred <- table(test_predicted, test_dat$tip)
print("Overall accuracy:")
mean(predict(nb_mod, test_dat) == test_dat$tip)
print("Test sensitivity and specificity:")
sensitivity(test_pred)
specificity(test_pred)
```
Interesting- this is the highest accuracy we've seen so far.

 
## LIME Analysis

LIME: Local Interpretable Model-agnostic Explanations. This fits a local model around a specific observation to explain why a black-box model (such as Naive Bayes) made the decision it did in classification. We'd like to look at a couple observations and look at the LIME visualization to get a deeper sense of how Naive Bayes worked behind the scenes.

```{r}
library(lime)
model_type.naiveBayes <- function(x){
  return("classification")
}

predict_model.naiveBayes <- function(x, newdata, type = "raw") {

    # return classification probabilities only   
    res <- predict(x, newdata, type = "raw") %>% as.data.frame()
    
    return(res)
}

test_dat_lime <- test_dat[sample(nrow(test_dat), 4),]
test_dat_lime
explainer <- lime(train_dat, as_classifier(nb_mod), bin_continuous = TRUE, quantile_bins = FALSE)

explanation <- lime::explain(test_dat_lime, explainer, n_labels = 1, n_features = 4)

explanation |> plot_features()

```

It seems that the top 4 feautures that contributed to the classification are payment_type, dropoff_borough, pickup_borough, and high_fare. Payment type is by far the most supportive of the classification type. Again, this is in line with what I discovered from my previous 3 models. In 3 of the 4 cases, the payment type is 1 (credit card), strongly supporting the classification 1 of the observation. In the 1 case in which the payment type is 2 (cash), it also strongly supports the classification of 0. In all 4 cases, the dropoff_borough is Manhattan. In the 3 cases in which the observation is classified as 1, this is supporting the classification, but in the one case in which the observation is classified as 0, this contradicts the classification. This supports the generalization that riders being dropped off in Manhattan tend to tip over 15%. 

