[["index.html", "NYC Yellow Taxi Tipping Patterns Chapter 1 Introduction 1.1 Data 1.2 Modeling Goal 1.3 Models 1.4 Data Sources", " NYC Yellow Taxi Tipping Patterns Vivian Liu 2023-05-06 Chapter 1 Introduction 1.1 Data In this project, I intend to use the 2021 Yellow Taxi Trip Data. I got the data from NYC’s OpenData source, linked here: https://data.cityofnewyork.us/Transportation/2021-Yellow-Taxi-Trip-Data/m6nq-qud6. I want to use the tip information, given as a “tip amount” column, and divide it by the “fare amount” column in order to create a tip rate. I plan to use the other predictors in the data (such as time of ride, passenger count, etc) to predict whether or not a given rider (or riders) tipped over 15% on their ride. 1.2 Modeling Goal I’d like to use different column values (borough, time of day, weekend vs not weekend, payment type) to predict whether or not the rider gives the driver a tip. Since tip amount is a continuous variable, instead I could categorize this variable as more or less than 15% tip (compared with the fare amount). 1.3 Models I intend to use the following models: OneR Decision Tree Random Forest Naive Bayes For each of these models, I will create visualizations to interpret my results, and ultimately produce accuracies and sensitivy/specifity values so as to be able to compare the models against each other in terms of prediction efficacy. 1.4 Data Sources https://catalog.data.gov/dataset/nyc-taxi-zones https://www.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf https://catalog.data.gov/dataset/nyc-taxi-zones "],["exploration.html", "Chapter 2 Exploration 2.1 Data Wrangling 2.2 Data Visualization", " Chapter 2 Exploration 2.1 Data Wrangling My goal is to look into the factors that determine how people tip. To do this, I want to create a “tipped” column that is just a binary of whether or not somebody tipped at all. I also want to create a “tip” column that shows whether or not somebody tipped over 15% of the fare amount. ## X VendorID tpep_pickup_datetime ## 0 132 0 ## tpep_dropoff_datetime passenger_count trip_distance ## 0 231 0 ## RatecodeID store_and_fwd_flag PULocationID ## 231 0 0 ## DOLocationID payment_type fare_amount ## 0 132 0 ## extra mta_tax tip_amount ## 0 0 0 ## tolls_amount improvement_surcharge total_amount ## 0 0 0 ## congestion_surcharge ## 99 To understand the columns and what the values mean, I used this site: https://www.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf It seems that most columns don’t have NAs. The ones that do are passenger_count, VendorID, RatecodeID, payment_type, and congestion_surcharge. Passenger_count: just remove the rows that don’t have this value VendorID: replace NA with 0 to represent unknown RatecodeID: not sure payment_type: replace all NA with code 5 (unknown) congestion_surcharge: not sure ## X VendorID tpep_pickup_datetime ## 0 0 0 ## tpep_dropoff_datetime passenger_count trip_distance ## 0 231 0 ## RatecodeID store_and_fwd_flag PULocationID ## 231 0 0 ## DOLocationID payment_type fare_amount ## 0 0 0 ## extra mta_tax tip_amount ## 0 0 0 ## tolls_amount improvement_surcharge total_amount ## 0 0 0 ## congestion_surcharge ## 99 For some reason, removing all rows with passenger_count NA removes all other NA instances. This is good! ## [1] 0.2672 ## [1] 4999 20 ## [1] 0.3516703 As we can see, of the sampled data, 27.1% of rides were NOT tipped and 29.6% of rides were tipped under 10% (including not tipped at all). Next, I want to get a general sense of the distribution of tip amounts and the distribution of fare amounts. This is odd- there seem to be some rides where the fare amount is negative (the taxi driver owes the passenger the fare amount). I did some research and attributed this to messy data, so I removed the negative fares and redid the histogram. We see that most fares seem to be around $5-20. I’d like to make a categorical variable for rides over and under $20. ## [1] 0.1622057 So around 14.4% of all rides are over $20. We see a spike at 0 representing the proportion of rides that weren’t tipped at all. Generally, we see that most rides are tipped between the $0-5 range. We see a spike at 0 representing the proportion of rides that aren’t tipped at all. We also see a spike around 25-30% tip. Next, I want to see how long these rides last. To do that, I need to first convert the pickup/dropoff times to time variables, and find the difference. In my exploration of the data, there were some values that seemed like outliers (rides that took multiple hours). I removed 11 of these rows from the data set. Next, I want to create a categorical variable for the season in which the ride took place based on the pickup time. To do this, I looked into this package: hydroTSM. Maybe the season in which the ride took place will impact the amount riders tip! Finally, I would like to create a categorical variable that takes into account the pickup and dropoff locations. I don’t want it to be too granular, so I’m using the “pulocationID” and “dolocationID” columns. Just looking at the numbers, they don’t mean anything so I looked into what the IDs mean. I found this dataset: https://catalog.data.gov/dataset/nyc-taxi-zones and named it pickup_ids in my code. I cross-referenced the “borough” column of pickup_ids to figure out the borough each pickup ID corresponds to. This link was also helpful. https://www.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf ## pickup_borough ## Bronx Brooklyn Manhattan Queens unknown ## 19 57 4499 328 55 ## dropoff_borough ## Bronx Brooklyn EWR Manhattan Queens ## 47 215 8 4400 249 ## Staten Island unknown ## 1 38 2.2 Data Visualization Let’s look at the relative frequencies of the passenger count. For the most part, we have rides with only 1 passenger, followed by 2. What’s surprising to me is that there are rides with 5-6 passengers at all; over covid, I remember that most taxis only allowed 1-3 passengers to ride but maybe by late 2021 they lifted the restrictions. ## ## 0 1 2 3 4 5 6 ## 107 3433 773 175 76 91 72 This is interesting- there doesn’t seem to be much of a trend in passenger count vs tip percentage. ## pickup_borough ## Bronx Brooklyn Manhattan Queens unknown ## 19 57 4499 328 55 At this point in my iterations, I’ve created all the new categorical variables I’d like to create for my data analysis. "],["model-1-oner.html", "Chapter 3 [Model 1: OneR] 3.1 With payment_type 3.2 Without payment_type", " Chapter 3 [Model 1: OneR] Split into test and train: In this model, I decided to use a OneR model to see which of the features would be most “important” in minimizing prediction error. How it works is that for each predictor that I feed the model, it generates “one rule,” and at the end it selects the rule that minimizes error. In this case, I decided to try pickup borough, dropoff borough, passenger count, high fare, season, rate code ID, payment_type, and vendor ID. pickup borough/dropoff borough: I thought this may be important in that people whose destinations/origins are different boroughs may have different tipping habits. passenger count: my hypothesis was that for rides with higher passenger counts, the tip might be higher. This was somewhat disproved in my EDA, but I wanted to include it in my OneR. high fare: I wanted to see whether or not rides with higher fares may tip higher, or if rides with lower fares tip higher proportionally to the fare. season: my hypothesis was that maybe rides in the winter (where it’s less convenient to walk/take public transporation) may be tipped higher. rate code ID: this represents different types of ride such as “standard” or rides to EWR/JFK. Maybe people who take rides with set fares (like those to the airports) consistently tip at 15-20% because they know exactly what to expect. payment type: either credit, cash, no charge, void, or unknown. vendor ID: this data represents two different taxi companies. Maybe one has a consistently higher level of service corresponding to better tips for their drivers. Above, I’ve listed the reasons for why I included specific features. But as a reminder, OneR will only choose ONE of the above predictors as the one that minimizes error. We’ll see which one does the best job when we run the model. 3.1 With payment_type ## ## Call: ## OneR.formula(formula = factor(tip) ~ pickup_borough + dropoff_borough + ## passenger_count + high_fare + season + RatecodeID + payment_type + ## VendorID, data = train_dat) ## ## Rules: ## If payment_type = 1 then factor(tip) = 1 ## If payment_type = 2 then factor(tip) = 0 ## If payment_type = 3 then factor(tip) = 0 ## If payment_type = 4 then factor(tip) = 0 ## ## Accuracy: ## 3351 of 3783 instances classified correctly (88.58%) ## ## Contingency table: ## payment_type ## factor(tip) 1 2 3 4 Sum ## 0 432 * 837 * 17 * 7 1293 ## 1 * 2490 0 0 0 2490 ## Sum 2922 837 17 7 3783 ## --- ## Maximum in each column: &#39;*&#39; ## ## Pearson&#39;s Chi-squared test: ## X-squared = 2146.6, df = 3, p-value &lt; 2.2e-16 Based on this plot, it seems that there’s only payment type 1 (credit card) that seems to want to tip. I didn’t expect this, but it somewhat makes sense. For example, at a coffee shop, I’m much more likely to tip when paying with a credit card since tapping the “15%” button on the screen is much easier than counting out change if I had paid with cash. Let’s do some EDA regarding payment type to see how this is distributed in the data. As we can see, those paying with cash didn’t tip at 15% or higher at all, which was surprising! I guess it’s hard to count out the exact tip by hand. Here, we can also see that the relative frequency of those paying with credit card was much higher than all other payment types combined. Let’s now take a look at the sensitivity, specificity, and overall accuracy of this model on the test data. ## [1] &quot;Overall accuracy:&quot; ## [1] 0.8316532 ## [1] &quot;Test sensitivity and specificity:&quot; ## [1] 0.6210191 ## [1] 1 3.2 Without payment_type Since payment_type seems to be the most important predictor, I wanted to see what the next most important predictor would be. So I decided to make another OneR model, leaving out payment_type and see which predictor it would settle on. ## ## Call: ## OneR.formula(formula = factor(tip) ~ pickup_borough + dropoff_borough + ## passenger_count + high_fare + season + RatecodeID + VendorID, ## data = train_dat) ## ## Rules: ## If dropoff_borough = Bronx then factor(tip) = 0 ## If dropoff_borough = Brooklyn then factor(tip) = 1 ## If dropoff_borough = EWR then factor(tip) = 0 ## If dropoff_borough = Manhattan then factor(tip) = 1 ## If dropoff_borough = Queens then factor(tip) = 1 ## If dropoff_borough = unknown then factor(tip) = 0 ## ## Accuracy: ## 2524 of 3783 instances classified correctly (66.72%) ## ## Contingency table: ## dropoff_borough ## factor(tip) Bronx Brooklyn EWR Manhattan Queens unknown Sum ## 0 * 29 60 * 3 1099 83 * 19 1293 ## 1 4 * 76 1 * 2305 * 92 12 2490 ## Sum 33 136 4 3404 175 31 3783 ## --- ## Maximum in each column: &#39;*&#39; ## ## Pearson&#39;s Chi-squared test: ## X-squared = 80.442, df = 5, p-value = 6.782e-16 ## [1] 0.6596067 ## [1] 0.6673387 Interestingly, I have an accuracy of 66% for the training data, and an even higher accuracy of 66.7% for the test data. Compared to an overall frequency of 64.8% for tipped &gt; 15%, this doesn’t seem like that much of an information gain and the test accuracy being higher is a little odd. Something interesting came up with the OneR function as I was running this. I noticed that for the Brooklyn column, OneR classified it as 1 when from my explorations, I knew that Brooklyn should have been classified as 0 since there were more instances of 0 than 1 in the data. As we can see, the relative frequencies for Brooklyn were 88 for 0 and 86 for 1, which means that OneR should have classified Brooklyn as 0 instead of 1 as it did in the model. However, OneR said there were 60 instances of 0 and 76 instances of 1 in the data. Why were some of the data points missing? Now that we’ve removed all the rows with NA, we see the same results that OneR showed in the summary. It turns out this was all because the OneR function has been tossing any rows with NAs in it. This was good to know, and maybe this should have been more clear in the documentation! Based on the OneR analysis, it seems that dropoff borough is the most important in determining whether or not a rider will tip over 15% on their ride, and it categorizes at 66.7% accuracy. However, we already know that 65% of all rides are classified as over 15% tip so we’re not sure how much information we’re gaining. As we can see from the plot, there are 5 categories (Bronx, Brooklyn, Manhattan, Queens, and unknown borough). Riders who are picked up in Manhattan have the highest rate of tipping over 15%, and riders who are picked up from the Bronx have the lowest rate of tipping over 15%. This made sense intuitively when I looked up the median household income for the different boroughs after making this plot. Highest to lowest median income goes Manhattan, Staten Island, Queens, Brooklyn, then Bronx which is the same order for tipping &gt;15% in this plot. For each borough, it’s showing the relative frequencies which shows that Manhattan has the most rides. It also shows very few data points for EWR, Bronx, and unknown. It’s possible that those with fewer points would be skewed towards more 0’s than 1’s so that’s something to keep in mind. ## [1] &quot;Test sensitivity and specificity:&quot; ## [1] 0.03264095 ## [1] 0.9938931 This model, being very simplistic, has a very low sensitivity and a high specificity. The values are much lower when we exclude payment_type! "],["model-2-decision-tree.html", "Chapter 4 [Model 2: Decision Tree] 4.1 Decision Tree Pruning 4.2 ROC “Curve”", " Chapter 4 [Model 2: Decision Tree] 4.1 Decision Tree Pruning For my second model, I decided to a decision tree model so I can see which features the model decides to split on. I’d like to compare the features in this model with the features that OneR ranked as important, and see if they line up. In addition, I’d like to see a more granular breakdown of the distributions of tip conditioned on different predictors. First, I want to split into test and train. For the same reasons as in model1, I decided to choose passenger_count, VendorID, RatecodeID, dropoff_borough, season, and high_fare as my predictors for my decision tree. However, this time I would also like to see if payment type has any influence on this as well. The different payment types are cash, credit, no charge, disputed, unknown, and void. These are categorical variables with easily interpretable meanings, so I decided to keep these as my predictors. This is our initial tree with maximum depth (CP=0), which is the maximum fitting we can do (with no pruning). As we can tell, it’s not very interpretable, as it splits on many variables and is very hard to understand. Before we try some pruning, let’s look at the accuracy of this model. ## [1] &quot;Test sensitivity and specificity:&quot; ## [1] 0.6697248 ## [1] 0.9919225 The sensitivity is not great, but the specificity is pretty good. order to create a more useful model for us, we can turn to pruning to cut down on the predictors the tree tries to split on. To do this, we can take a look at the CP table, which will give us information as to the CP value that gives us the minimal error. ## [1] 0.001529052 It seems that the optimal cp value is 0.001529052, which is higher than our original cp=0. Hopefully, once we raise the cp value the resulting tree will be smaller and easier to interpret. This is really interesting- and much easier to understand! Now that we include payment type, we see that credit card rides are overwhelmingly tipped oer 15%. This confirms what we discovered in the OneR model. On the other side, it seems that “cash, no charge, or dispute” was overwhelmingly tipped under 15%. If we keep going on the right side branch, high_fare is quite important. We also see that the dropoff_borough is not as important of a branching factor as in the OneR model. The OneR grouped together Bronx, EWR, Queens, and unknown into one group, and Brooklyn, Manhattan into another, whereas our “optimal” decision tree has decided this as a later branching factor with a different combination. Finally, let’s look at the accuracy, specificity, and sensitivity of the pruned tree as compared to the original unpruned tree. ## [1] &quot;Overall accuracy:&quot; ## [1] 0.8837209 ## [1] &quot;Test sensitivity and specificity:&quot; ## [1] 0.6697248 ## [1] 0.9983845 The sensitivity and specificity for the test set on the pruned tree is very slightly better than that of the unpruned tree. However, it is important to note that the sensitivity (true positive rate) is still quite low, while the specificity (true negative rate) is quite high. We don’t gain that much from pruning. 4.2 ROC “Curve” Let’s get another view on this using an ROC curve. ## geom_label_repel: parse = FALSE, box.padding = 0.25, label.padding = 0.25, point.padding = 1e-06, label.r = 0.15, label.size = 0.25, min.segment.length = 0.5, arrow = NULL, na.rm = FALSE, force = 1, force_pull = 1, max.time = 0.5, max.iter = 10000, max.overlaps = 10, nudge_x = 0, nudge_y = 0, xlim = c(NA, NA), ylim = c(NA, NA), direction = both, seed = NA, verbose = FALSE ## stat_identity: na.rm = FALSE ## position_identity This is a very interesting ROC curve. This seems to indicate that the predictions of the model that I used are just predicting 1 (tipped over 15%) with a probability of 67% for almost every row. The 67% reflects the proportion of the total dataset that tipped over 15%, so it seems that the constructed model isn’t very useful. "],["model-3-random-forest.html", "Chapter 5 [Model 3: Random Forest] 5.1 Random Forest: PU/DOLocationID vs pickup/dropoff_borough 5.2 Shapley Value Analysis", " Chapter 5 [Model 3: Random Forest] 5.1 Random Forest: PU/DOLocationID vs pickup/dropoff_borough In this model, I decided to try the original, raw PU/DOLocationID columns instead of the pickup/dropoff_borough columns I created and compare the performance of the two. PU/DOLocationID gives us the exact taxi zone from which passengers were picked up/dropped off (there are about 200 of these covered New York City), adding further granularity to the location information represented in the column, whereas pickup/dropoff_borough gives us the borough. The reason I originally chose to create pickup/dropoff_borough was that it gave us 5 easy to interpret categories; “Queens,” “Manhattan,” “Bronx” etc was much more understandable by a reader than taxi zones such as “192,” “4,” and “95” that give us no immediate information in where they are located geographically. Expanding to PU/DOLocationID will be more computationally heavy, since there are now 200 categories to deal with, but my hypothesis is that it will give the model a little more information to work with. ## [1] 0.8837209 To understand our random forest results, we first need to understand OOB (out of bag) error. Each bagged tree uses about 2/3 of the observations, and the remaining 1/3 of observations is referred to as the out of bag observations. Since in this case we care about classification, we take the majority vote for each observation, in which each prediction is made using the 1/3 OOB observations. This is a valid estimate of the test error for the bagged model. The 11.77% number is a weighted average based on how many observations go through that node. Note: we can also confirm that it’s a good estimate for error because the OOB estimate (11.77) + mean prediction accuracy (88.37) roughly equals 1. So far it’s also doing better than OneR and Decision Tree! The accuracy on the test data is 88.37%, which is considerably better than just guessing 1 for all the rows (which would yield about 67% accuracy). Next, I’d like to do the same model, replacing DULocationID with dropoff_borough and PULocationID with pickup_borough to see if my hypothesis is correct. ## [1] 0.8879493 The OOB error is 11.43% which is slightly better than the random forest using the location ID! The prediction accuracy on the test data is 88.79% which is also slightly better than when using the locationID. Looks like my hypothesis was incorrect- using just the borough as predictors worked just as well (in fact slightly better) than having the more granular location ID. Another possibility is that dropoff borough was not a very important predictor in the random forest model, so switching it out slightly didn’t make much of an impact on the model accuracy. Looking at the plotted random forest model, we see that the error rate doesn’t change after 30 trees, indicating that 30 trees is probably enough for our purposes. In terms of error, our error for classifications of 1 is quite low, whereas the error for classifications of 1 is quite high. This may again be indicative of the relative frequencies of 0’s and 1’s (the data is roughly 35% 0’s and 65% 1’s), so the model may be blanket classifying rows as 1 leading to high 0 classification error. Next, let’s take a look at the accuracy, sensitivity, and specificity. ## [1] &quot;Overall accuracy:&quot; ## [1] 0.8879493 ## [1] &quot;Test sensitivity and specificity:&quot; ## [1] 0.6727829 ## [1] 1 Let’s also take a look at feature importance based on our random forest model. Just as my other models confirmed, payment_type is by far the best predictor, whereas the other predictors are hovering around the 0.0 line. After looking at 3 models that confirm this, it seems that taxi drivers should really start encouraging their customers to pay with card as often as possible! 5.2 Shapley Value Analysis I’d also like to look at the Shapley values. Shapley is a permutation method, looks at all combinations of features, which is costly in terms of computation. However, since I’ve decided not to use the 200+ categories for PU/DOLocationID (instead opting for the 5 category pickup/dropoff_borough), I think it’s an appropriate thing to analyze. Let’s do 100 simulations to explain the 100th observation in the data! ## # A tibble: 1 × 7 ## passenger_count VendorID RatecodeID dropoff_bor…¹ season high_fare paymen…² ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -3.48e-19 5.34e-18 0 -1.87e-17 -1.18e-17 -3.43e-19 1.64e-18 ## # … with abbreviated variable names ¹​dropoff_borough, ²​payment_type ## # A tibble: 7 × 2 ## var shap_value ## &lt;chr&gt; &lt;dbl&gt; ## 1 passenger_count -3.48e-19 ## 2 VendorID 5.34e-18 ## 3 RatecodeID 0 ## 4 dropoff_borough -1.87e-17 ## 5 season -1.18e-17 ## 6 high_fare -3.43e-19 ## 7 payment_type 1.64e-18 From our shapley values, we can tell that payment type contributed the most positive values in the prediction for the 100th observation, whereas dropoff_borough contributed the most negative values in the prediction. Let’s take a look at the 100th observation to see if the shapley values line up with what we’d expect! ## X.1 X VendorID tpep_pickup_datetime tpep_dropoff_datetime ## 2204 2225 18305441 1 09/07/2021 02:12:41 PM 09/07/2021 02:20:42 PM ## passenger_count trip_distance RatecodeID store_and_fwd_flag PULocationID ## 2204 1 1 1 N 236 ## DOLocationID payment_type fare_amount extra mta_tax tip_amount ## 2204 237 1 7.5 2.5 0.5 2.15 ## tolls_amount improvement_surcharge total_amount congestion_surcharge ## 2204 0 0.3 12.95 2.5 ## tipped tip_percent tip high_fare pickup dropoff ## 2204 1 0.2866667 1 0 2021-09-07 14:12:41 2021-09-07 14:20:42 ## duration season pickup_borough dropoff_borough ## 2204 8.016667 autumm Manhattan Manhattan We see that the payment_type was 1, or credit card, which we’ve established is indicative of a much higher rate of tipping. We also see that the dropoff borough is Manhattan. This is surprising, because from our OneR analysis we’d expect tha Manhattan dropoff would more likely be a tip percentage of &gt;15%. However, when we look at our feature importance plot from earlier, we found that actually, dropoff borough isn’t as important as it seems, and may not even be much more important than any of the other features. The passenger count is 1, vendorID is 1 (Creative Mobile Technologies), season is autumn, is not high_fare, and ratecodeID is 1 indicating standard rate. Overall, payment_type seems to contribute the most to the prediction which is in line with what we’d expect in our feature importance plot. "],["model-4-naive-bayes.html", "Chapter 6 [Model 4: Naive Bayes] 6.1 Naive Bayes Model", " Chapter 6 [Model 4: Naive Bayes] 6.1 Naive Bayes Model For my final model, I wanted to see how Naive Bayes would work. It seems like a pretty simple predictive model, so I wanted to see how it may compare to my other models. For this model, I’ll just produce the model with the same features I’ve been using so far and get the accuracy, sensitivity, and specificity. ## ## nb_mod_pred 0 1 ## 0 222 1 ## 1 105 618 ## [1] &quot;Overall accuracy:&quot; ## [1] 0.8879493 ## [1] &quot;Test sensitivity and specificity:&quot; ## [1] 0.6788991 ## [1] 0.9983845 Interesting- this is the highest accuracy we’ve seen so far. "],["reflections.html", "Chapter 7 Reflections 7.1 Model Conclusions 7.2 Final Thoughts 7.3 References", " Chapter 7 Reflections 7.1 Model Conclusions This was a very fun dataset to work with! It required quite a bit of cleaning and data wrangling in the beginning, but I got to create some fun columns that hopefully made the data easier to understand for someone new to the dataset. Some of the columns that I created included season (looking at the date the ride occured), pickup/dropoff_borough (cross-referenceing the taxi zone with its borough), and high_fare (is the fare over $20). At the end of each model I created, I also looked into the sensitivity and specificity values so I could get another metric that I could compare different models against each other. Here, I’ve summarized the overall accuracy/sensitivity/specificity values. OneR: 0.8826638 / 0.6210191 / 1 Decision Tree: 0.8837209 / 0.6697248 / 0.9983845 Random Forest: 0.8868922 / 0.6727829 / 1 Naive Bayes: 0.8879493 / 0.6788991 / 0.9983845 In terms of model interpretability, I found that OneR and DecisionTree yielded the lowest accuracies (not by far at all), but were the easiest to interpret. On the other hand, RandomForest and Naive Bayes did very slightly better, but was harder to interpret the results. Although OneR and DecisionTree didn’t do as well, I felt it was necessary to understand all of the models so I could get a holistic understanding of the important predictors of the data. 7.2 Final Thoughts Overall, the models that I created worked alright to predict whether or not someone would tip over 15% or not. For all the models, I had an accuracy of around 88% that I wasn’t able to make higher despite trying different combinations of features and adding features. There may be a few reasons for this, but I think the most important of the reasons was that the data did not actually give us an good information about the rider. Some information such as demographics, maybe yearly income, even personality were obviously not included for each ride, and these predictors may have been very helpful in predicting our outcome variable. Instead, we got information such as VendorID (one of 2 taxi companies represented in the data), whether or not the ride was on a tolled path, and the time at which it occurred. The closest approximation for demographic information might have been dropoff_borough, as the boroughs can be ranked on median income. dropoff_borough also happened to be an important predictor across the board for all of the models. Another thing that was interesting was that when I tried using the original PU/DOLocationID columns in the models instead, the accuracy didn’t go up even though the pickup/dropoff location information was more granular. The computation time went way up (since we went from 5 borough categories to around 200 taxi zone categories), but the accuracy didn’t change much. The data is probably very good for someone looking to start their own taxi company and wanting to look at trends in where/when rides occur, but in terms of predicting tipping pattern, the companies would need to start collecting demographic information about each ride which is quite impractical. Based on my analysis, my recommendation for taxi drivers in NYC is to encourage their riders to pay using card as often as possible! 7.3 References Here are a list of sources that I used to help me with my code and data interpretation: https://cran.r-project.org/web/packages/hydroTSM/index.html https://stackoverflow.com/questions/20328452/legend-for-random-forest-plot-in-r https://stat.ethz.ch/R-manual/R-devel/library/rpart/html/predict.rpart.html http://www.sthda.com/english/articles/35-statistical-machine-learning-essentials/141-cart-model-decision-tree-essentials/#:~:text=In%20rpart%20package%2C%20this%20is,to%20a%20too%20small%20tree. https://statinfer.com/203-4-2-calculating-sensitivity-and-specificity-in-r/ https://algotech.netlify.app/blog/text-lime/ An Introduction to Statistical Learning (James, Witten, Hastie, Tibshirani) pg. 342-343 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
