[["index.html", "Final Project Proposal Chapter 1 Proposal", " Final Project Proposal Vivian Liu 2023-05-01 Chapter 1 Proposal The data: 2021 Yellow Taxi Trip Data (30M rows) The modeling goal: I’d like to use different column values (borough, time of day, weekend vs not weekend, payment type) to predict whether or not the rider gives the driver a tip. Since tip amount is a continuous variable, instead I could categorize this variable as more or less than 15% tip (compared with the fare amount). The methods or models you intend to use (at least three): I intend to use linear regression, k-nearest neighbors, and random forest. For k-nearest neighbors, I will probably use a variety of distance metrics between different rides, including cosine similarity, Euclidean distance, and Minkowski distance. For linear regression, I am curious to know which column will have the greatest effect on tip amount. I will regress against single variables, combinations of variables, and all variables to see which column explains the most variance in the results. Finally, I would like to use random forest for classification to classify trips as more or less than 15% tip. Shapley value Use random forest to predict the amount of tip, given calendar and borough information Shapley is a permutation method, looks at all combinations of features (costly in terms of computation) Fit linear model for SHAP by minimizing the loss function "],["sources.html", "Chapter 2 Sources", " Chapter 2 Sources https://catalog.data.gov/dataset/nyc-taxi-zones https://www.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf https://cran.r-project.org/web/packages/hydroTSM/index.html "],["exploration.html", "Chapter 3 Exploration", " Chapter 3 Exploration ## X VendorID tpep_pickup_datetime tpep_dropoff_datetime ## 1 26182255 2 11/17/2021 11:52:23 PM 11/18/2021 12:04:05 AM ## 2 27741282 1 12/01/2021 02:16:36 PM 12/01/2021 02:38:44 PM ## 3 12283324 2 07/02/2021 09:32:26 AM 07/02/2021 09:40:17 AM ## 4 12193005 2 07/01/2021 09:51:47 AM 07/01/2021 10:01:16 AM ## 5 20402360 1 09/29/2021 07:13:41 AM 09/29/2021 07:18:02 AM ## 6 22447780 2 10/16/2021 10:58:35 PM 10/16/2021 11:02:27 PM ## passenger_count trip_distance RatecodeID store_and_fwd_flag PULocationID ## 1 1 1.67 1 N 48 ## 2 1 4.30 1 N 239 ## 3 2 1.19 1 N 230 ## 4 1 1.86 1 N 142 ## 5 1 0.70 1 N 42 ## 6 1 0.86 1 N 229 ## DOLocationID payment_type fare_amount extra mta_tax tip_amount tolls_amount ## 1 234 2 9.0 0.5 0.5 0.00 0 ## 2 170 1 18.5 2.5 0.5 4.35 0 ## 3 162 1 6.5 0.0 0.5 1.96 0 ## 4 100 1 8.5 0.0 0.5 2.36 0 ## 5 42 1 5.0 0.0 0.5 1.70 0 ## 6 162 2 5.0 0.5 0.5 0.00 0 ## improvement_surcharge total_amount congestion_surcharge ## 1 0.3 12.80 2.5 ## 2 0.3 26.15 2.5 ## 3 0.3 11.76 2.5 ## 4 0.3 14.16 2.5 ## 5 0.3 7.50 0.0 ## 6 0.3 8.80 2.5 My goal is to look into the factors that determine how people tip. To do this, I want to create a “tipped” column that is just a binary of whether or not somebody tipped at all. I also want to create a “tip” column that shows whether or not somebody tipped over 10% of the fare amount. Note: initially, I tried to use 15% as the threshold. After doing some analysis on results and thinking about them, I decided that 10% would be a better threshold as it would indicate one tipping above a negligable amount. Usually, 15% is a pretty standard tip for some who meant to tip at all. ## X VendorID tpep_pickup_datetime ## 0 132 0 ## tpep_dropoff_datetime passenger_count trip_distance ## 0 231 0 ## RatecodeID store_and_fwd_flag PULocationID ## 231 0 0 ## DOLocationID payment_type fare_amount ## 0 132 0 ## extra mta_tax tip_amount ## 0 0 0 ## tolls_amount improvement_surcharge total_amount ## 0 0 0 ## congestion_surcharge ## 99 To understand the columns and what the values mean, I used this site: https://www.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf It seems that most columns don’t have NAs. The ones that do are passenger_count, VendorID, RatecodeID, payment_type, and congestion_surcharge. Passenger_count: just remove the rows that don’t have this value VendorID: replace NA with 0 to represent unknown RatecodeID: not sure payment_type: replace all NA with code 5 (unknown) congestion_surcharge: not sure ## X VendorID tpep_pickup_datetime ## 0 0 0 ## tpep_dropoff_datetime passenger_count trip_distance ## 0 231 0 ## RatecodeID store_and_fwd_flag PULocationID ## 231 0 0 ## DOLocationID payment_type fare_amount ## 0 0 0 ## extra mta_tax tip_amount ## 0 0 0 ## tolls_amount improvement_surcharge total_amount ## 0 0 0 ## congestion_surcharge ## 99 For some reason, removing all rows with passenger_count NA removes all other NA instances. This is good! ## [1] 0.2672 ## [1] 4999 20 ## [1] 0.3516703 As we can see, of the sampled data, 27.1% of rides were NOT tipped and 29.6% of rides were tipped under 10% (including not tipped at all). Next, I want to get a general sense of the distribution of tip amounts and the distribution of fare amounts. ## X VendorID tpep_pickup_datetime tpep_dropoff_datetime ## 25 4444895 2 03/30/2021 03:12:08 PM 03/30/2021 03:12:17 PM ## 139 5535112 2 04/14/2021 04:20:05 PM 04/14/2021 04:25:06 PM ## 165 2497690 2 02/25/2021 09:51:50 PM 02/25/2021 09:52:20 PM ## 166 16072754 2 08/13/2021 01:47:46 PM 08/13/2021 01:58:08 PM ## 811 24960480 2 11/07/2021 11:24:24 AM 11/07/2021 11:34:57 AM ## 881 1986604 2 02/15/2021 09:58:58 AM 02/15/2021 10:06:46 AM ## 899 13615844 2 07/17/2021 10:27:44 PM 07/17/2021 11:05:04 PM ## 931 17303474 2 08/27/2021 06:04:10 PM 08/27/2021 06:05:15 PM ## 1000 23267558 2 10/24/2021 03:26:57 PM 10/24/2021 03:28:13 PM ## 1018 10574509 2 06/14/2021 10:20:37 PM 06/14/2021 10:21:17 PM ## 1039 579917 2 01/15/2021 03:51:17 PM 01/15/2021 03:52:56 PM ## 1117 199355 2 01/06/2021 05:22:13 PM 01/06/2021 05:26:49 PM ## 1550 7202182 2 05/06/2021 12:05:19 AM 05/06/2021 12:05:21 AM ## 1684 479464 2 01/13/2021 02:00:16 PM 01/13/2021 02:00:44 PM ## 1698 27757367 2 12/01/2021 04:21:14 PM 12/01/2021 04:32:22 PM ## 1818 12490945 2 07/05/2021 11:02:39 AM 07/05/2021 11:05:00 AM ## 2538 21514118 2 10/08/2021 11:10:27 AM 10/08/2021 11:20:10 AM ## 2680 21230043 2 10/05/2021 06:58:30 PM 10/05/2021 07:01:51 PM ## 2906 22408528 2 10/16/2021 05:18:42 PM 10/16/2021 05:21:30 PM ## 3120 27129642 2 11/26/2021 07:38:06 PM 11/26/2021 07:41:24 PM ## 3134 12301741 2 07/02/2021 01:43:46 PM 07/02/2021 01:44:14 PM ## 3305 27488431 2 11/30/2021 01:54:43 PM 11/30/2021 02:25:37 PM ## 3428 27989983 2 12/03/2021 02:05:00 PM 12/03/2021 02:18:42 PM ## 3438 30024910 2 12/21/2021 08:47:07 AM 12/21/2021 08:47:29 AM ## 3903 25579290 2 11/12/2021 08:49:59 PM 11/12/2021 08:50:14 PM ## 3917 10131688 2 06/09/2021 09:50:23 PM 06/09/2021 09:51:28 PM ## 4080 23389908 2 10/25/2021 06:43:59 PM 10/25/2021 06:44:09 PM ## 4108 25316099 2 11/10/2021 05:46:48 PM 11/10/2021 05:46:58 PM ## 4257 9520423 2 06/03/2021 07:19:18 AM 06/03/2021 07:23:18 AM ## 4400 12367828 2 07/03/2021 10:44:03 AM 07/03/2021 10:54:25 AM ## passenger_count trip_distance RatecodeID store_and_fwd_flag PULocationID ## 25 2 0.01 2 N 236 ## 139 2 0.63 1 N 237 ## 165 4 0.03 1 N 151 ## 166 1 4.63 1 N 132 ## 811 1 2.75 1 N 193 ## 881 1 2.09 1 N 249 ## 899 1 17.13 2 N 132 ## 931 1 0.13 3 N 132 ## 1000 2 0.18 1 N 41 ## 1018 2 0.00 1 N 141 ## 1039 1 0.31 1 N 263 ## 1117 1 0.86 1 N 161 ## 1550 1 0.03 1 N 264 ## 1684 1 0.00 1 N 193 ## 1698 1 1.06 1 N 113 ## 1818 1 0.92 1 N 75 ## 2538 1 1.24 1 N 230 ## 2680 1 0.34 1 N 249 ## 2906 1 0.53 1 N 238 ## 3120 1 0.68 1 N 114 ## 3134 2 0.03 1 N 230 ## 3305 1 9.98 1 N 138 ## 3428 2 3.88 1 N 132 ## 3438 6 0.00 1 N 152 ## 3903 1 0.00 1 N 186 ## 3917 1 0.03 1 N 234 ## 4080 1 0.00 5 N 237 ## 4108 1 0.03 1 N 239 ## 4257 1 0.61 1 N 143 ## 4400 1 1.92 1 N 48 ## DOLocationID payment_type fare_amount extra mta_tax tip_amount ## 25 236 2 -52.0 0.0 -0.5 0.00 ## 139 163 3 -5.0 -1.0 -0.5 0.00 ## 165 151 3 -2.5 -0.5 -0.5 0.00 ## 166 215 4 -14.5 0.0 -0.5 0.00 ## 811 145 4 -10.0 0.0 -0.5 0.00 ## 881 246 3 -9.0 0.0 -0.5 0.00 ## 899 234 3 -52.0 0.0 -0.5 0.00 ## 931 132 3 -20.5 -1.0 0.0 0.00 ## 1000 41 4 -3.0 0.0 -0.5 0.00 ## 1018 141 2 -2.5 -0.5 -0.5 0.00 ## 1039 263 4 -3.0 0.0 -0.5 0.00 ## 1117 141 4 -5.0 -1.0 -0.5 0.00 ## 1550 264 4 -2.5 -0.5 -0.5 0.00 ## 1684 193 3 -2.5 0.0 -0.5 -0.66 ## 1698 246 4 -8.5 -1.0 -0.5 0.00 ## 1818 263 3 -4.5 0.0 -0.5 1.00 ## 2538 246 4 -8.0 0.0 -0.5 0.00 ## 2680 249 3 -3.0 -1.0 -0.5 0.00 ## 2906 151 4 -4.0 0.0 -0.5 0.00 ## 3120 113 4 -4.5 -1.0 -0.5 0.00 ## 3134 230 3 -2.5 0.0 -0.5 0.00 ## 3305 162 4 -31.0 0.0 -0.5 8.17 ## 3428 215 2 -13.5 0.0 -0.5 0.00 ## 3438 152 3 -2.5 0.0 -0.5 0.00 ## 3903 186 3 -2.5 -0.5 -0.5 0.00 ## 3917 234 4 -3.0 -0.5 -0.5 0.00 ## 4080 237 3 -43.1 0.0 0.0 0.00 ## 4108 239 3 -2.5 -1.0 -0.5 0.00 ## 4257 239 4 -5.0 0.0 -0.5 0.00 ## 4400 229 4 -9.0 0.0 -0.5 0.00 ## tolls_amount improvement_surcharge total_amount congestion_surcharge ## 25 0.00 -0.3 -55.30 -2.5 ## 139 0.00 -0.3 -9.30 -2.5 ## 165 0.00 -0.3 -3.80 0.0 ## 166 0.00 -0.3 -16.55 0.0 ## 811 0.00 -0.3 -10.80 0.0 ## 881 0.00 -0.3 -12.30 -2.5 ## 899 -6.55 -0.3 -63.10 -2.5 ## 931 0.00 -0.3 -23.05 0.0 ## 1000 0.00 -0.3 -3.80 0.0 ## 1018 0.00 -0.3 -6.30 -2.5 ## 1039 0.00 -0.3 -6.30 -2.5 ## 1117 0.00 -0.3 -9.30 -2.5 ## 1550 0.00 -0.3 -3.80 0.0 ## 1684 0.00 -0.3 -3.96 0.0 ## 1698 0.00 -0.3 -12.80 -2.5 ## 1818 0.00 -0.3 -6.80 -2.5 ## 2538 0.00 -0.3 -11.30 -2.5 ## 2680 0.00 -0.3 -7.30 -2.5 ## 2906 0.00 -0.3 -7.30 -2.5 ## 3120 0.00 -0.3 -8.80 -2.5 ## 3134 0.00 -0.3 -5.80 -2.5 ## 3305 -6.55 -0.3 -33.93 -2.5 ## 3428 0.00 -0.3 -15.55 0.0 ## 3438 0.00 -0.3 -3.30 0.0 ## 3903 0.00 -0.3 -6.30 -2.5 ## 3917 0.00 -0.3 -6.80 -2.5 ## 4080 0.00 -0.3 -45.90 -2.5 ## 4108 0.00 -0.3 -6.80 -2.5 ## 4257 0.00 -0.3 -8.30 -2.5 ## 4400 0.00 -0.3 -12.30 -2.5 ## tipped tip_percent tip ## 25 0 0.0000000 0 ## 139 0 0.0000000 0 ## 165 0 0.0000000 0 ## 166 0 0.0000000 0 ## 811 0 0.0000000 0 ## 881 0 0.0000000 0 ## 899 0 0.0000000 0 ## 931 0 0.0000000 0 ## 1000 0 0.0000000 0 ## 1018 0 0.0000000 0 ## 1039 0 0.0000000 0 ## 1117 0 0.0000000 0 ## 1550 0 0.0000000 0 ## 1684 1 0.2640000 1 ## 1698 0 0.0000000 0 ## 1818 1 -0.2222222 0 ## 2538 0 0.0000000 0 ## 2680 0 0.0000000 0 ## 2906 0 0.0000000 0 ## 3120 0 0.0000000 0 ## 3134 0 0.0000000 0 ## 3305 1 -0.2635484 0 ## 3428 0 0.0000000 0 ## 3438 0 0.0000000 0 ## 3903 0 0.0000000 0 ## 3917 0 0.0000000 0 ## 4080 0 0.0000000 0 ## 4108 0 0.0000000 0 ## 4257 0 0.0000000 0 ## 4400 0 0.0000000 0 This is odd- there seem to be some rides where the fare amount is negative (the taxi driver owes the passenger the fare amount). I did some research and attributed this to messy data, so I removed the negative fares and redid the histogram. We see that most fares seem to be around $5-20. I’d like to make a categorical variable for rides over and under $20. ## [1] 0.1622057 So around 14.4% of all rides are over $20. We see a spike at 0 representing the proportion of rides that weren’t tipped at all. Generally, we see that most rides are tipped between the $0-5 range. We see a spike at 0 representing the proportion of rides that aren’t tipped at all. We also see a spike around 25-30% tip. Next, I want to see how long these rides last. To do that, I need to first convert the pickup/dropoff times to time variables, and find the difference. In my exploration of the data, there were some values that seemed like outliers (rides that took multiple hours). I removed 11 of these rows from the data set. Next, I want to create a categorical variable for the season in which the ride took place based on the pickup time. To do this, I will looked into this package: hydroTSM. Maybe the season in which the ride took place will impact the amount riders tip! Finally, I would like to create a categorical variable that takes into account the pickup and dropoff locations. I don’t want it to be too granular, so I’m using the “pulocationID” and “dolocationID” columns. Just looking at the numbers, they don’t mean anything so I looked into what the IDs mean. I found this dataset: https://catalog.data.gov/dataset/nyc-taxi-zones and named it pickup_ids in my code. I cross-referenced the “borough” column of pickup_ids to figure out the borough each pickup ID corresponds to. This link was also helpful. https://www.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf ## pickup_borough ## Bronx Brooklyn Manhattan Queens unknown ## 19 57 4499 328 55 ## dropoff_borough ## Bronx Brooklyn EWR Manhattan Queens ## 47 215 8 4400 249 ## Staten Island unknown ## 1 38 Let’s look at the relative frequencies of the passenger count. For the most part, we have rides with only 1 passenger, followed by 2. What’s surprising to me is that there are rides with 5-6 passengers at all; over covid, I remember that most taxis only allowed 1-3 passengers to ride but maybe by late 2021 they lifted the restrictions. ## ## 0 1 2 3 4 5 6 ## 107 3433 773 175 76 91 72 This is interesting- there doesn’t seem to be much of a trend in passenger count vs tip percentage. ## pickup_borough ## Bronx Brooklyn Manhattan Queens unknown ## 19 57 4499 328 55 At this point in my iterations, I’ve created all the new categorical variables I’d like to create for my data analysis. "],["model-1.html", "Chapter 4 [Model 1]", " Chapter 4 [Model 1] Split into test and train: ## X.1 X VendorID tpep_pickup_datetime tpep_dropoff_datetime ## 2885 2909 12405440 2 07/03/2021 07:28:51 PM 07/03/2021 07:37:38 PM ## 10 10 7142653 2 05/05/2021 11:30:48 AM 05/05/2021 11:50:01 AM ## 3208 3234 18732634 2 09/11/2021 07:30:46 PM 09/11/2021 07:46:37 PM ## 3649 3678 29380007 2 12/15/2021 07:52:46 AM 12/15/2021 08:16:21 AM ## 4464 4502 25171931 2 11/09/2021 02:29:21 PM 11/09/2021 03:03:43 PM ## 735 741 25126790 2 11/09/2021 06:40:41 AM 11/09/2021 06:48:09 AM ## passenger_count trip_distance RatecodeID store_and_fwd_flag PULocationID ## 2885 1 0.00 1 N 231 ## 10 1 2.38 1 N 236 ## 3208 2 1.57 1 N 231 ## 3649 1 2.58 1 N 162 ## 4464 1 3.64 1 N 163 ## 735 1 1.85 1 N 239 ## DOLocationID payment_type fare_amount extra mta_tax tip_amount ## 2885 246 1 6.5 0 0.5 1.47 ## 10 164 1 13.5 0 0.5 5.04 ## 3208 79 1 11.0 0 0.5 2.86 ## 3649 239 1 15.5 0 0.5 2.82 ## 4464 163 1 22.0 0 0.5 25.30 ## 735 230 1 8.0 0 0.5 2.00 ## tolls_amount improvement_surcharge total_amount congestion_surcharge ## 2885 0 0.3 11.27 2.5 ## 10 0 0.3 21.84 2.5 ## 3208 0 0.3 17.16 2.5 ## 3649 0 0.3 21.62 2.5 ## 4464 0 0.3 50.60 2.5 ## 735 0 0.3 13.30 2.5 ## tipped tip_percent tip high_fare pickup dropoff ## 2885 1 0.2261538 1 0 2021-07-03 19:28:51 2021-07-03 19:37:38 ## 10 1 0.3733333 1 0 2021-05-05 11:30:48 2021-05-05 11:50:01 ## 3208 1 0.2600000 1 0 2021-09-11 19:30:46 2021-09-11 19:46:37 ## 3649 1 0.1819355 1 0 2021-12-15 07:52:46 2021-12-15 08:16:21 ## 4464 1 1.1500000 1 1 2021-11-09 14:29:21 2021-11-09 15:03:43 ## 735 1 0.2500000 1 0 2021-11-09 06:40:41 2021-11-09 06:48:09 ## duration season pickup_borough dropoff_borough ## 2885 8.783333 summer Manhattan Manhattan ## 10 19.216667 spring Manhattan Manhattan ## 3208 15.850000 autumm Manhattan Manhattan ## 3649 23.583333 winter Manhattan Manhattan ## 4464 34.366667 autumm Manhattan Manhattan ## 735 7.466667 autumm Manhattan Manhattan In this model, I decided to use a OneR model to see which of the features would be most “important” in minimizing prediction error. How it works is that for each predictor that I feed the model, it generates “one rule,” and at the end it selects the rule that minimizes error. In this case, I decided to try pickup borough, dropoff borough, passenger count, high fare, season, rate code ID, and vendor ID. pickup borough/dropoff borough: I thought this may be important in that people whose destinations/origins are different boroughs may have different tipping habits. passenger count: my hypothesis was that for rides with higher passenger counts, the tip might be higher. This was somewhat disproved in my EDA, but I wanted to include it in my OneR. high fare: I wanted to see whether or not rides with higher fares may tip higher, or if rides with lower fares tip higher proportionally to the fare. season: my hypothesis was that maybe rides in the winter (where it’s less convenient to walk/take public transporation) may be tipped higher. rate code ID: this represents different types of ride such as “standard” or rides to EWR/JFK. Maybe people who take rides with set fares (like those to the airports) consistently tip at 15-20% because they know exactly what to expect. vendor ID: this data represents two different taxi companies. Maybe one has a consistently higher level of service corresponding to better tips for their drivers. Above, I’ve listed the reasons for why I included specific features. But as a reminder, OneR will only choose ONE of the above predictors as the one that minimizes error. We’ll see which one does the best job when we run the model. ## ## Call: ## OneR.formula(formula = factor(tip) ~ pickup_borough + dropoff_borough + ## passenger_count + high_fare + season + RatecodeID + VendorID, ## data = train_dat) ## ## Rules: ## If dropoff_borough = Bronx then factor(tip) = 0 ## If dropoff_borough = Brooklyn then factor(tip) = 1 ## If dropoff_borough = EWR then factor(tip) = 0 ## If dropoff_borough = Manhattan then factor(tip) = 1 ## If dropoff_borough = Queens then factor(tip) = 1 ## If dropoff_borough = unknown then factor(tip) = 0 ## ## Accuracy: ## 2524 of 3783 instances classified correctly (66.72%) ## ## Contingency table: ## dropoff_borough ## factor(tip) Bronx Brooklyn EWR Manhattan Queens unknown Sum ## 0 * 29 60 * 3 1099 83 * 19 1293 ## 1 4 * 76 1 * 2305 * 92 12 2490 ## Sum 33 136 4 3404 175 31 3783 ## --- ## Maximum in each column: &#39;*&#39; ## ## Pearson&#39;s Chi-squared test: ## X-squared = 80.442, df = 5, p-value = 6.782e-16 ## [1] 0.6596067 ## [1] 0.6673387 Interestingly, I have an accuracy of 66% for the training data, and an even higher accuracy of 66.7% for the test data. Compared to an overall frequency of 64.8% for tipped &gt; 15%, this doesn’t seem like that much of an information gain and the test accuracy being higher is a little odd. Something interesting came up with the OneR function as I was running this. I noticed that for the Brooklyn column, OneR classified it as 1 when from my explorations, I knew that Brooklyn should have been classified as 0 since there were more instances of 0 than 1 in the data. As we can see, the relative frequencies for Brooklyn were 88 for 0 and 86 for 1, which means that OneR should have classified Brooklyn as 0 instead of 1 as it did in the model. However, OneR said there were 60 instances of 0 and 76 instances of 1 in the data. Why were some of the data points missing? Now that we’ve removed all the rows with NA, we see the same results that OneR showed in the summary. It turns out this was all because the OneR function has been tossing any rows with NAs in it. This was good to know, and maybe this should have been more clear in the documentation! Based on the OneR analysis, it seems that dropoff borough is the most important in determining whether or not a rider will tip over 15% on their ride, and it categorizes at 66.7% accuracy. However, we already know that 65% of all rides are classified as over 15% tip so we’re not sure how much information we’re gaining. As we can see from the plot, there are 5 categories (Bronx, Brooklyn, Manhattan, Queens, and unknown borough). Riders who are picked up in Manhattan have the highest rate of tipping over 15%, and riders who are picked up from the Bronx have the lowest rate of tipping over 15%. This made sense intuitively when I looked up the median household income for the different boroughs after making this plot. Highest to lowest median income goes Manhattan, Staten Island, Queens, Brooklyn, then Bronx which is the same order for tipping &gt;15% in this plot. For each borough, it’s showing the relative frequencies which shows that Manhattan has the most rides. It also shows very few data points for EWR, Bronx, and unknown. It’s possible that those with fewer points would be skewed towards more 0’s than 1’s so that’s something to keep in mind. "],["model-2.html", "Chapter 5 [Model 2]", " Chapter 5 [Model 2] For my second model, I decided to try removing all rows with passenger counts that are NA. Miraculously, all NAs in the entire dataset disappeared! I will be using this set going forward. Now, I want to split into test and train: ## Call: ## rpart(formula = tip ~ passenger_count + VendorID + RatecodeID + ## dropoff_borough + season + high_fare, data = train_dat, method = &quot;class&quot;, ## control = rpart.control(cp = 0)) ## n= 3781 ## ## CP nsplit rel error xerror xstd ## 1 0.0132812500 0 1.0000000 1.0000000 0.02273257 ## 2 0.0062500000 1 0.9867188 1.0000000 0.02273257 ## 3 0.0054687500 2 0.9804688 1.0000000 0.02273257 ## 4 0.0046875000 4 0.9695313 1.0000000 0.02273257 ## 5 0.0023437500 5 0.9648438 0.9984375 0.02272388 ## 6 0.0018229167 6 0.9625000 1.0062500 0.02276700 ## 7 0.0015625000 9 0.9570312 1.0078125 0.02277553 ## 8 0.0013020833 11 0.9539063 1.0023438 0.02274554 ## 9 0.0011718750 14 0.9500000 1.0023438 0.02274554 ## 10 0.0007812500 16 0.9476563 1.0039062 0.02275415 ## 11 0.0003906250 24 0.9414062 1.0015625 0.02274122 ## 12 0.0002604167 30 0.9390625 1.0031250 0.02274985 ## 13 0.0000000000 33 0.9382813 1.0054688 0.02276272 ## ## Variable importance ## dropoff_borough RatecodeID season passenger_count high_fare ## 43 19 16 11 7 ## VendorID ## 4 ## ## Node number 1: 3781 observations, complexity param=0.01328125 ## predicted class=1 expected loss=0.3385348 P(node) =1 ## class counts: 1280 2501 ## probabilities: 0.339 0.661 ## left son=2 (235 obs) right son=3 (3546 obs) ## Primary splits: ## dropoff_borough splits as LRLRLL, improve=19.574710, (0 missing) ## high_fare &lt; 0.5 to the right, improve=11.362510, (0 missing) ## RatecodeID &lt; 2.5 to the right, improve= 5.856027, (0 missing) ## passenger_count &lt; 0.5 to the right, improve= 1.008938, (0 missing) ## VendorID &lt; 1.5 to the left, improve= 0.334281, (0 missing) ## Surrogate splits: ## RatecodeID &lt; 2.5 to the right, agree=0.94, adj=0.03, (0 split) ## ## Node number 2: 235 observations, complexity param=0.00546875 ## predicted class=0 expected loss=0.4638298 P(node) =0.06215287 ## class counts: 126 109 ## probabilities: 0.536 0.464 ## left son=4 (32 obs) right son=5 (203 obs) ## Primary splits: ## dropoff_borough splits as L-R-RR, improve=7.0091830, (0 missing) ## RatecodeID &lt; 1.5 to the left, improve=3.6313050, (0 missing) ## high_fare &lt; 0.5 to the left, improve=1.3639840, (0 missing) ## season splits as RLRR, improve=1.0282250, (0 missing) ## passenger_count &lt; 4.5 to the left, improve=0.6321611, (0 missing) ## ## Node number 3: 3546 observations, complexity param=0.00625 ## predicted class=1 expected loss=0.3254371 P(node) =0.9378471 ## class counts: 1154 2392 ## probabilities: 0.325 0.675 ## left son=6 (10 obs) right son=7 (3536 obs) ## Primary splits: ## RatecodeID &lt; 2.5 to the right, improve=6.6211220, (0 missing) ## high_fare &lt; 0.5 to the right, improve=6.0724450, (0 missing) ## dropoff_borough splits as -L-R--, improve=1.8593150, (0 missing) ## passenger_count &lt; 0.5 to the right, improve=1.2191900, (0 missing) ## VendorID &lt; 1.5 to the left, improve=0.2611971, (0 missing) ## ## Node number 4: 32 observations, complexity param=0.00078125 ## predicted class=0 expected loss=0.15625 P(node) =0.008463369 ## class counts: 27 5 ## probabilities: 0.844 0.156 ## left son=8 (25 obs) right son=9 (7 obs) ## Primary splits: ## season splits as RLLL, improve=3.088929000, (0 missing) ## high_fare &lt; 0.5 to the left, improve=0.710227300, (0 missing) ## VendorID &lt; 1.5 to the left, improve=0.008928571, (0 missing) ## ## Node number 5: 203 observations, complexity param=0.00546875 ## predicted class=1 expected loss=0.4876847 P(node) =0.0536895 ## class counts: 99 104 ## probabilities: 0.488 0.512 ## left son=10 (159 obs) right son=11 (44 obs) ## Primary splits: ## RatecodeID &lt; 1.5 to the left, improve=2.4204130, (0 missing) ## high_fare &lt; 0.5 to the left, improve=1.0279060, (0 missing) ## dropoff_borough splits as --R-RL, improve=0.9725392, (0 missing) ## passenger_count &lt; 2.5 to the left, improve=0.7593679, (0 missing) ## season splits as LLRL, improve=0.4864676, (0 missing) ## Surrogate splits: ## dropoff_borough splits as --R-LL, agree=0.803, adj=0.091, (0 split) ## ## Node number 6: 10 observations ## predicted class=0 expected loss=0.1 P(node) =0.002644803 ## class counts: 9 1 ## probabilities: 0.900 0.100 ## ## Node number 7: 3536 observations, complexity param=0.001822917 ## predicted class=1 expected loss=0.3238122 P(node) =0.9352023 ## class counts: 1145 2391 ## probabilities: 0.324 0.676 ## left son=14 (395 obs) right son=15 (3141 obs) ## Primary splits: ## high_fare &lt; 0.5 to the right, improve=5.1622860, (0 missing) ## dropoff_borough splits as -L-R--, improve=1.7123350, (0 missing) ## RatecodeID &lt; 1.5 to the right, improve=1.6748600, (0 missing) ## passenger_count &lt; 0.5 to the right, improve=1.1749510, (0 missing) ## VendorID &lt; 1.5 to the left, improve=0.3417962, (0 missing) ## Surrogate splits: ## RatecodeID &lt; 1.5 to the right, agree=0.909, adj=0.182, (0 split) ## dropoff_borough splits as -L-R--, agree=0.906, adj=0.157, (0 split) ## ## Node number 8: 25 observations ## predicted class=0 expected loss=0.04 P(node) =0.006612007 ## class counts: 24 1 ## probabilities: 0.960 0.040 ## ## Node number 9: 7 observations ## predicted class=1 expected loss=0.4285714 P(node) =0.001851362 ## class counts: 3 4 ## probabilities: 0.429 0.571 ## ## Node number 10: 159 observations, complexity param=0.0046875 ## predicted class=0 expected loss=0.4716981 P(node) =0.04205237 ## class counts: 84 75 ## probabilities: 0.528 0.472 ## left son=20 (105 obs) right son=21 (54 obs) ## Primary splits: ## season splits as LLRL, improve=1.1500450, (0 missing) ## passenger_count &lt; 2.5 to the left, improve=0.7890182, (0 missing) ## high_fare &lt; 0.5 to the left, improve=0.3157056, (0 missing) ## VendorID &lt; 1.5 to the right, improve=0.2180499, (0 missing) ## dropoff_borough splits as ----RL, improve=0.1367495, (0 missing) ## ## Node number 11: 44 observations, complexity param=0.00234375 ## predicted class=1 expected loss=0.3409091 P(node) =0.01163713 ## class counts: 15 29 ## probabilities: 0.341 0.659 ## left son=22 (13 obs) right son=23 (31 obs) ## Primary splits: ## dropoff_borough splits as --L-RL, improve=2.7801710, (0 missing) ## RatecodeID &lt; 3.5 to the right, improve=1.4402210, (0 missing) ## passenger_count &lt; 1.5 to the right, improve=0.6530692, (0 missing) ## VendorID &lt; 1.5 to the left, improve=0.1893939, (0 missing) ## season splits as LRLR, improve=0.1213112, (0 missing) ## Surrogate splits: ## RatecodeID &lt; 2.5 to the right, agree=0.886, adj=0.615, (0 split) ## ## Node number 14: 395 observations, complexity param=0.001822917 ## predicted class=1 expected loss=0.4 P(node) =0.1044697 ## class counts: 158 237 ## probabilities: 0.400 0.600 ## left son=28 (127 obs) right son=29 (268 obs) ## Primary splits: ## VendorID &lt; 1.5 to the left, improve=2.41484300, (0 missing) ## dropoff_borough splits as -L-R--, improve=0.32276810, (0 missing) ## passenger_count &lt; 1.5 to the left, improve=0.20724080, (0 missing) ## RatecodeID &lt; 1.5 to the right, improve=0.16441350, (0 missing) ## season splits as LLRR, improve=0.09802594, (0 missing) ## Surrogate splits: ## passenger_count &lt; 0.5 to the left, agree=0.709, adj=0.094, (0 split) ## ## Node number 15: 3141 observations, complexity param=0.000390625 ## predicted class=1 expected loss=0.3142311 P(node) =0.8307326 ## class counts: 987 2154 ## probabilities: 0.314 0.686 ## left son=30 (3077 obs) right son=31 (64 obs) ## Primary splits: ## passenger_count &lt; 0.5 to the right, improve=1.6129710000, (0 missing) ## season splits as RLLR, improve=0.1043094000, (0 missing) ## VendorID &lt; 1.5 to the left, improve=0.0012775340, (0 missing) ## dropoff_borough splits as -R-L--, improve=0.0001937715, (0 missing) ## ## Node number 20: 105 observations, complexity param=0.0015625 ## predicted class=0 expected loss=0.4285714 P(node) =0.02777043 ## class counts: 60 45 ## probabilities: 0.571 0.429 ## left son=40 (12 obs) right son=41 (93 obs) ## Primary splits: ## dropoff_borough splits as ----RL, improve=0.86405530, (0 missing) ## passenger_count &lt; 4 to the left, improve=0.66826220, (0 missing) ## high_fare &lt; 0.5 to the left, improve=0.30612240, (0 missing) ## VendorID &lt; 1.5 to the right, improve=0.10901660, (0 missing) ## season splits as LR-R, improve=0.02506266, (0 missing) ## ## Node number 21: 54 observations, complexity param=0.00078125 ## predicted class=1 expected loss=0.4444444 P(node) =0.01428194 ## class counts: 24 30 ## probabilities: 0.444 0.556 ## left son=42 (40 obs) right son=43 (14 obs) ## Primary splits: ## VendorID &lt; 1.5 to the right, improve=0.288095200, (0 missing) ## passenger_count &lt; 2.5 to the left, improve=0.090579710, (0 missing) ## high_fare &lt; 0.5 to the left, improve=0.007575758, (0 missing) ## ## Node number 22: 13 observations ## predicted class=0 expected loss=0.3846154 P(node) =0.003438244 ## class counts: 8 5 ## probabilities: 0.615 0.385 ## ## Node number 23: 31 observations ## predicted class=1 expected loss=0.2258065 P(node) =0.008198889 ## class counts: 7 24 ## probabilities: 0.226 0.774 ## ## Node number 28: 127 observations, complexity param=0.001822917 ## predicted class=1 expected loss=0.480315 P(node) =0.033589 ## class counts: 61 66 ## probabilities: 0.480 0.520 ## left son=56 (93 obs) right son=57 (34 obs) ## Primary splits: ## season splits as LLLR, improve=2.28266300, (0 missing) ## passenger_count &lt; 2.5 to the right, improve=0.58652780, (0 missing) ## RatecodeID &lt; 1.5 to the right, improve=0.09519565, (0 missing) ## dropoff_borough splits as -R-L--, improve=0.05188536, (0 missing) ## ## Node number 29: 268 observations, complexity param=0.00078125 ## predicted class=1 expected loss=0.3619403 P(node) =0.07088072 ## class counts: 97 171 ## probabilities: 0.362 0.638 ## left son=58 (204 obs) right son=59 (64 obs) ## Primary splits: ## passenger_count &lt; 1.5 to the left, improve=1.0948570, (0 missing) ## dropoff_borough splits as -L-R--, improve=0.5776783, (0 missing) ## season splits as RRRL, improve=0.4422420, (0 missing) ## RatecodeID &lt; 1.5 to the right, improve=0.1150179, (0 missing) ## ## Node number 30: 3077 observations, complexity param=0.000390625 ## predicted class=1 expected loss=0.3165421 P(node) =0.8138059 ## class counts: 974 2103 ## probabilities: 0.317 0.683 ## left son=60 (784 obs) right son=61 (2293 obs) ## Primary splits: ## passenger_count &lt; 1.5 to the right, improve=1.213904000, (0 missing) ## VendorID &lt; 1.5 to the left, improve=0.102197800, (0 missing) ## season splits as RLLR, improve=0.101199700, (0 missing) ## dropoff_borough splits as -L-R--, improve=0.002283788, (0 missing) ## ## Node number 31: 64 observations ## predicted class=1 expected loss=0.203125 P(node) =0.01692674 ## class counts: 13 51 ## probabilities: 0.203 0.797 ## ## Node number 40: 12 observations ## predicted class=0 expected loss=0.25 P(node) =0.003173764 ## class counts: 9 3 ## probabilities: 0.750 0.250 ## ## Node number 41: 93 observations, complexity param=0.0015625 ## predicted class=0 expected loss=0.4516129 P(node) =0.02459667 ## class counts: 51 42 ## probabilities: 0.548 0.452 ## left son=82 (85 obs) right son=83 (8 obs) ## Primary splits: ## passenger_count &lt; 2.5 to the left, improve=1.5586340, (0 missing) ## high_fare &lt; 0.5 to the left, improve=0.2297583, (0 missing) ## VendorID &lt; 1.5 to the right, improve=0.2073733, (0 missing) ## season splits as RR-L, improve=0.1205506, (0 missing) ## ## Node number 42: 40 observations, complexity param=0.00078125 ## predicted class=1 expected loss=0.475 P(node) =0.01057921 ## class counts: 19 21 ## probabilities: 0.475 0.525 ## left son=84 (16 obs) right son=85 (24 obs) ## Primary splits: ## high_fare &lt; 0.5 to the left, improve=0.4083333, (0 missing) ## passenger_count &lt; 1.5 to the right, improve=0.1506270, (0 missing) ## Surrogate splits: ## dropoff_borough splits as ----RL, agree=0.675, adj=0.188, (0 split) ## passenger_count &lt; 3.5 to the right, agree=0.625, adj=0.063, (0 split) ## ## Node number 43: 14 observations ## predicted class=1 expected loss=0.3571429 P(node) =0.003702724 ## class counts: 5 9 ## probabilities: 0.357 0.643 ## ## Node number 56: 93 observations, complexity param=0.001302083 ## predicted class=0 expected loss=0.4623656 P(node) =0.02459667 ## class counts: 50 43 ## probabilities: 0.538 0.462 ## left son=112 (9 obs) right son=113 (84 obs) ## Primary splits: ## passenger_count &lt; 2.5 to the right, improve=1.1492580, (0 missing) ## dropoff_borough splits as -R-L--, improve=0.4179598, (0 missing) ## RatecodeID &lt; 1.5 to the right, improve=0.3649497, (0 missing) ## season splits as LRL-, improve=0.2154411, (0 missing) ## ## Node number 57: 34 observations ## predicted class=1 expected loss=0.3235294 P(node) =0.00899233 ## class counts: 11 23 ## probabilities: 0.324 0.676 ## ## Node number 58: 204 observations, complexity param=0.00078125 ## predicted class=1 expected loss=0.3872549 P(node) =0.05395398 ## class counts: 79 125 ## probabilities: 0.387 0.613 ## left son=116 (132 obs) right son=117 (72 obs) ## Primary splits: ## season splits as LLRL, improve=0.6470588, (0 missing) ## RatecodeID &lt; 1.5 to the right, improve=0.5666667, (0 missing) ## dropoff_borough splits as -L-R--, improve=0.3169306, (0 missing) ## ## Node number 59: 64 observations, complexity param=0.0002604167 ## predicted class=1 expected loss=0.28125 P(node) =0.01692674 ## class counts: 18 46 ## probabilities: 0.281 0.719 ## left son=118 (35 obs) right son=119 (29 obs) ## Primary splits: ## season splits as RLLL, improve=1.25628100, (0 missing) ## passenger_count &lt; 2.5 to the left, improve=0.27032160, (0 missing) ## dropoff_borough splits as -L-R--, improve=0.18031730, (0 missing) ## RatecodeID &lt; 1.5 to the left, improve=0.09777847, (0 missing) ## Surrogate splits: ## passenger_count &lt; 5.5 to the left, agree=0.562, adj=0.034, (0 split) ## ## Node number 60: 784 observations, complexity param=0.000390625 ## predicted class=1 expected loss=0.3405612 P(node) =0.2073526 ## class counts: 267 517 ## probabilities: 0.341 0.659 ## left son=120 (632 obs) right son=121 (152 obs) ## Primary splits: ## season splits as LLLR, improve=0.7470683, (0 missing) ## VendorID &lt; 1.5 to the right, improve=0.1919206, (0 missing) ## dropoff_borough splits as -R-L--, improve=0.1325742, (0 missing) ## passenger_count &lt; 3.5 to the right, improve=0.1282307, (0 missing) ## ## Node number 61: 2293 observations ## predicted class=1 expected loss=0.3083297 P(node) =0.6064533 ## class counts: 707 1586 ## probabilities: 0.308 0.692 ## ## Node number 82: 85 observations, complexity param=0.001171875 ## predicted class=0 expected loss=0.4235294 P(node) =0.02248083 ## class counts: 49 36 ## probabilities: 0.576 0.424 ## left son=164 (55 obs) right son=165 (30 obs) ## Primary splits: ## VendorID &lt; 1.5 to the right, improve=0.5422460000, (0 missing) ## high_fare &lt; 0.5 to the left, improve=0.1498733000, (0 missing) ## season splits as RR-L, improve=0.0392156900, (0 missing) ## passenger_count &lt; 1.5 to the left, improve=0.0008521719, (0 missing) ## Surrogate splits: ## passenger_count &lt; 0.5 to the right, agree=0.706, adj=0.167, (0 split) ## ## Node number 83: 8 observations ## predicted class=1 expected loss=0.25 P(node) =0.002115842 ## class counts: 2 6 ## probabilities: 0.250 0.750 ## ## Node number 84: 16 observations ## predicted class=0 expected loss=0.4375 P(node) =0.004231685 ## class counts: 9 7 ## probabilities: 0.562 0.438 ## ## Node number 85: 24 observations ## predicted class=1 expected loss=0.4166667 P(node) =0.006347527 ## class counts: 10 14 ## probabilities: 0.417 0.583 ## ## Node number 112: 9 observations ## predicted class=0 expected loss=0.2222222 P(node) =0.002380323 ## class counts: 7 2 ## probabilities: 0.778 0.222 ## ## Node number 113: 84 observations, complexity param=0.001302083 ## predicted class=0 expected loss=0.4880952 P(node) =0.02221634 ## class counts: 43 41 ## probabilities: 0.512 0.488 ## left son=226 (64 obs) right son=227 (20 obs) ## Primary splits: ## season splits as LRL-, improve=0.2011905, (0 missing) ## RatecodeID &lt; 1.5 to the right, improve=0.1428571, (0 missing) ## passenger_count &lt; 0.5 to the right, improve=0.1060606, (0 missing) ## dropoff_borough splits as -R-L--, improve=0.0717001, (0 missing) ## ## Node number 116: 132 observations, complexity param=0.00078125 ## predicted class=1 expected loss=0.4166667 P(node) =0.0349114 ## class counts: 55 77 ## probabilities: 0.417 0.583 ## left son=232 (26 obs) right son=233 (106 obs) ## Primary splits: ## dropoff_borough splits as -L-R--, improve=0.96057090, (0 missing) ## season splits as LR-L, improve=0.10784310, (0 missing) ## RatecodeID &lt; 1.5 to the right, improve=0.03358255, (0 missing) ## ## Node number 117: 72 observations, complexity param=0.00078125 ## predicted class=1 expected loss=0.3333333 P(node) =0.01904258 ## class counts: 24 48 ## probabilities: 0.333 0.667 ## left son=234 (9 obs) right son=235 (63 obs) ## Primary splits: ## RatecodeID &lt; 1.5 to the right, improve=1.01587300, (0 missing) ## dropoff_borough splits as -R-L--, improve=0.01454545, (0 missing) ## ## Node number 118: 35 observations, complexity param=0.0002604167 ## predicted class=1 expected loss=0.3714286 P(node) =0.00925681 ## class counts: 13 22 ## probabilities: 0.371 0.629 ## left son=236 (26 obs) right son=237 (9 obs) ## Primary splits: ## RatecodeID &lt; 1.5 to the left, improve=1.6420020, (0 missing) ## dropoff_borough splits as -L-R--, improve=0.7000000, (0 missing) ## passenger_count &lt; 2.5 to the left, improve=0.3125541, (0 missing) ## season splits as -LRL, improve=0.1075630, (0 missing) ## ## Node number 119: 29 observations ## predicted class=1 expected loss=0.1724138 P(node) =0.007669929 ## class counts: 5 24 ## probabilities: 0.172 0.828 ## ## Node number 120: 632 observations, complexity param=0.000390625 ## predicted class=1 expected loss=0.3512658 P(node) =0.1671515 ## class counts: 222 410 ## probabilities: 0.351 0.649 ## left son=240 (463 obs) right son=241 (169 obs) ## Primary splits: ## VendorID &lt; 1.5 to the right, improve=0.18279810, (0 missing) ## passenger_count &lt; 4.5 to the right, improve=0.03129019, (0 missing) ## season splits as RLL-, improve=0.02133136, (0 missing) ## ## Node number 121: 152 observations ## predicted class=1 expected loss=0.2960526 P(node) =0.04020101 ## class counts: 45 107 ## probabilities: 0.296 0.704 ## ## Node number 164: 55 observations ## predicted class=0 expected loss=0.3818182 P(node) =0.01454642 ## class counts: 34 21 ## probabilities: 0.618 0.382 ## ## Node number 165: 30 observations, complexity param=0.001171875 ## predicted class=0 expected loss=0.5 P(node) =0.007934409 ## class counts: 15 15 ## probabilities: 0.500 0.500 ## left son=330 (7 obs) right son=331 (23 obs) ## Primary splits: ## season splits as RR-L, improve=0.8385093, (0 missing) ## high_fare &lt; 0.5 to the left, improve=0.0678733, (0 missing) ## Surrogate splits: ## passenger_count &lt; 1.5 to the right, agree=0.8, adj=0.143, (0 split) ## ## Node number 226: 64 observations, complexity param=0.001302083 ## predicted class=0 expected loss=0.46875 P(node) =0.01692674 ## class counts: 34 30 ## probabilities: 0.531 0.469 ## left son=452 (47 obs) right son=453 (17 obs) ## Primary splits: ## dropoff_borough splits as -R-L--, improve=0.6609825000, (0 missing) ## RatecodeID &lt; 1.5 to the right, improve=0.0053602060, (0 missing) ## season splits as R-L-, improve=0.0004901961, (0 missing) ## passenger_count &lt; 1.5 to the left, improve=0.0001564456, (0 missing) ## ## Node number 227: 20 observations ## predicted class=1 expected loss=0.45 P(node) =0.005289606 ## class counts: 9 11 ## probabilities: 0.450 0.550 ## ## Node number 232: 26 observations, complexity param=0.00078125 ## predicted class=0 expected loss=0.4615385 P(node) =0.006876488 ## class counts: 14 12 ## probabilities: 0.538 0.462 ## left son=464 (14 obs) right son=465 (12 obs) ## Primary splits: ## season splits as RL-L, improve=0.6611722, (0 missing) ## ## Node number 233: 106 observations ## predicted class=1 expected loss=0.3867925 P(node) =0.02803491 ## class counts: 41 65 ## probabilities: 0.387 0.613 ## ## Node number 234: 9 observations ## predicted class=0 expected loss=0.4444444 P(node) =0.002380323 ## class counts: 5 4 ## probabilities: 0.556 0.444 ## ## Node number 235: 63 observations ## predicted class=1 expected loss=0.3015873 P(node) =0.01666226 ## class counts: 19 44 ## probabilities: 0.302 0.698 ## ## Node number 236: 26 observations, complexity param=0.0002604167 ## predicted class=1 expected loss=0.4615385 P(node) =0.006876488 ## class counts: 12 14 ## probabilities: 0.462 0.538 ## left son=472 (7 obs) right son=473 (19 obs) ## Primary splits: ## dropoff_borough splits as -L-R--, improve=0.23134760, (0 missing) ## season splits as -LRL, improve=0.08974359, (0 missing) ## passenger_count &lt; 2.5 to the left, improve=0.02082128, (0 missing) ## Surrogate splits: ## season splits as -RRL, agree=0.769, adj=0.143, (0 split) ## ## Node number 237: 9 observations ## predicted class=1 expected loss=0.1111111 P(node) =0.002380323 ## class counts: 1 8 ## probabilities: 0.111 0.889 ## ## Node number 240: 463 observations ## predicted class=1 expected loss=0.3585313 P(node) =0.1224544 ## class counts: 166 297 ## probabilities: 0.359 0.641 ## ## Node number 241: 169 observations, complexity param=0.000390625 ## predicted class=1 expected loss=0.3313609 P(node) =0.04469717 ## class counts: 56 113 ## probabilities: 0.331 0.669 ## left son=482 (112 obs) right son=483 (57 obs) ## Primary splits: ## season splits as LLR-, improve=1.2647670, (0 missing) ## passenger_count &lt; 3.5 to the right, improve=0.8682191, (0 missing) ## Surrogate splits: ## dropoff_borough splits as -R-L--, agree=0.675, adj=0.035, (0 split) ## ## Node number 330: 7 observations ## predicted class=0 expected loss=0.2857143 P(node) =0.001851362 ## class counts: 5 2 ## probabilities: 0.714 0.286 ## ## Node number 331: 23 observations ## predicted class=1 expected loss=0.4347826 P(node) =0.006083047 ## class counts: 10 13 ## probabilities: 0.435 0.565 ## ## Node number 452: 47 observations ## predicted class=0 expected loss=0.4255319 P(node) =0.01243057 ## class counts: 27 20 ## probabilities: 0.574 0.426 ## ## Node number 453: 17 observations ## predicted class=1 expected loss=0.4117647 P(node) =0.004496165 ## class counts: 7 10 ## probabilities: 0.412 0.588 ## ## Node number 464: 14 observations ## predicted class=0 expected loss=0.3571429 P(node) =0.003702724 ## class counts: 9 5 ## probabilities: 0.643 0.357 ## ## Node number 465: 12 observations ## predicted class=1 expected loss=0.4166667 P(node) =0.003173764 ## class counts: 5 7 ## probabilities: 0.417 0.583 ## ## Node number 472: 7 observations ## predicted class=0 expected loss=0.4285714 P(node) =0.001851362 ## class counts: 4 3 ## probabilities: 0.571 0.429 ## ## Node number 473: 19 observations ## predicted class=1 expected loss=0.4210526 P(node) =0.005025126 ## class counts: 8 11 ## probabilities: 0.421 0.579 ## ## Node number 482: 112 observations, complexity param=0.000390625 ## predicted class=1 expected loss=0.375 P(node) =0.02962179 ## class counts: 42 70 ## probabilities: 0.375 0.625 ## left son=964 (9 obs) right son=965 (103 obs) ## Primary splits: ## passenger_count &lt; 3.5 to the right, improve=1.665049, (0 missing) ## ## Node number 483: 57 observations ## predicted class=1 expected loss=0.245614 P(node) =0.01507538 ## class counts: 14 43 ## probabilities: 0.246 0.754 ## ## Node number 964: 9 observations ## predicted class=0 expected loss=0.3333333 P(node) =0.002380323 ## class counts: 6 3 ## probabilities: 0.667 0.333 ## ## Node number 965: 103 observations ## predicted class=1 expected loss=0.3495146 P(node) =0.02724147 ## class counts: 36 67 ## probabilities: 0.350 0.650 "],["model-3.html", "Chapter 6 [Model 3]", " Chapter 6 [Model 3] Split into test and train: ## X.1 X VendorID tpep_pickup_datetime tpep_dropoff_datetime ## 3030 3054 24238615 1 11/01/2021 10:22:16 AM 11/01/2021 10:30:11 AM ## 10 10 7142653 2 05/05/2021 11:30:48 AM 05/05/2021 11:50:01 AM ## 3361 3388 3113944 2 03/07/2021 05:56:54 PM 03/07/2021 06:15:10 PM ## 3822 3852 19495551 1 09/19/2021 05:32:54 PM 09/19/2021 05:47:13 PM ## 4685 4724 23093912 1 10/22/2021 10:58:17 PM 10/22/2021 11:22:20 PM ## 771 777 1588380 2 02/06/2021 03:29:28 PM 02/06/2021 03:36:18 PM ## passenger_count trip_distance RatecodeID store_and_fwd_flag PULocationID ## 3030 1 1.00 1 N 140 ## 10 1 2.38 1 N 236 ## 3361 1 4.36 1 N 43 ## 3822 1 5.00 1 N 87 ## 4685 2 6.30 1 N 25 ## 771 1 0.93 1 N 43 ## DOLocationID payment_type fare_amount extra mta_tax tip_amount ## 3030 237 1 7.0 2.5 0.5 2.55 ## 10 164 1 13.5 0.0 0.5 5.04 ## 3361 107 1 16.0 0.0 0.5 0.00 ## 3822 162 2 16.5 2.5 0.5 0.00 ## 4685 36 1 22.0 0.5 0.5 5.80 ## 771 263 1 6.5 0.0 0.5 2.94 ## tolls_amount improvement_surcharge total_amount congestion_surcharge ## 3030 0 0.3 12.85 2.5 ## 10 0 0.3 21.84 2.5 ## 3361 0 0.3 19.30 2.5 ## 3822 0 0.3 19.80 2.5 ## 4685 0 0.3 29.10 0.0 ## 771 0 0.3 12.74 2.5 ## tipped tip_percent tip high_fare pickup dropoff ## 3030 1 0.3642857 1 0 2021-11-01 10:22:16 2021-11-01 10:30:11 ## 10 1 0.3733333 1 0 2021-05-05 11:30:48 2021-05-05 11:50:01 ## 3361 0 0.0000000 0 0 2021-03-07 17:56:54 2021-03-07 18:15:10 ## 3822 0 0.0000000 0 0 2021-09-19 17:32:54 2021-09-19 17:47:13 ## 4685 1 0.2636364 1 1 2021-10-22 22:58:17 2021-10-22 23:22:20 ## 771 1 0.4523077 1 0 2021-02-06 15:29:28 2021-02-06 15:36:18 ## duration season pickup_borough dropoff_borough ## 3030 7.916667 autumm Manhattan Manhattan ## 10 19.216667 spring Manhattan Manhattan ## 3361 18.266667 spring Manhattan Manhattan ## 3822 14.316667 autumm Manhattan Manhattan ## 4685 24.050000 autumm Brooklyn Brooklyn ## 771 6.833333 winter Manhattan Manhattan ## ## Call: ## randomForest(formula = tip_01 ~ passenger_count + VendorID + RatecodeID + DOLocationID + PULocationID + season + high_fare, data = train_dat) ## Type of random forest: classification ## Number of trees: 500 ## No. of variables tried at each split: 2 ## ## OOB estimate of error rate: 25.73% ## Confusion matrix: ## 0 1 class.error ## 0 16 957 0.983556012 ## 1 16 2792 0.005698006 ## [1] 0.7367865 OOB (out of bag) estimate of error rate: at each split does some evaluation by splitting the data. The 25.9% number is a weighted average based on how many observations go through that node. Again, the random forest An Introduction to Statistical Learning pp. 342-343 (resource about randomforest OOB) ## Error in predict(model, newdata = newdata, type = &quot;prob&quot;)[, &quot;tip&quot;]: subscript out of bounds ## Error in as_tibble(shap_values): object &#39;shap_values&#39; not found ## Error in eval(expr, envir, enclos): object &#39;shap_values&#39; not found ## [1] 0.8733462 "],["reflections.html", "Chapter 7 Reflections", " Chapter 7 Reflections "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
